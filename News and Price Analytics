# -*- coding: utf-8 -*-
"""
Created on Sun May 23 09:15:01 2021

@author: micud


url = 'https://www.fool.co.uk/investing/2021/05/21/the-trainline-share-price-steadies-after-thursdays-20-crash-should-i-buy/'

article = Article(url)

#Do some NLP
article.download()
article.parse()

article.nlp()

#Get article text
article_text = article.text

#Get the author
author_list = article.authors

#Get publish date
publish_date = article.publish_date

#Get top image
image = article.top_image



Key packages used:
pygooglenews = reliable package to scrape URL results to overcome beautiful soup issues on my end. https://github.com/kotartemiy/pygooglenews
beautifulsoup = used to scrape text in webpages
nltk = 
financial-summarization-pegasus = Deep Learning model that has been pretrained specifically for Financial summarization

"""
import nltk
from newspaper import Article
#nltk.download('punkt')
nltk.download('punkt')
import pandas as pd
from bs4 import BeautifulSoup
import requests
from pygooglenews import GoogleNews
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from transformers import pipeline
import re
from io import StringIO
import csv
from pandas.tseries.offsets import BDay
import numpy as np


#Specify Watchlist
#watchlist = ['Sumo Digital','Ocado','IQE','Trainline','Dotdigital']

watchlist = {'Sumo Group Plc':'SUMO.L','Ocado':'OCDO.L','IQE':'IQE.L','Trainline':'TRN.L','Dotdigital':'DOTD.L','NCC plc':'NCC.L','MusicMagpie':'MMAG.L','Royal Mail':'RMG.L','Oxford Instruments plc':'OXIG.L'}


#Specify Region
gn = GoogleNews(country = 'UK')



#Setup AI Model using Pegasus which has been trained for Financial Summarization
model_name = "human-centered-summarization/financial-summarization-pegasus"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)
summarizer = pipeline("summarization")



#This will be the final dataframe output where results for each company will be appended
master_df = []

for company_name, ticker in watchlist.items():
    
    #company_name = 'Trainline'
    #ticker = 'TRN.L'
    
    
    #Get 1year Daily Stock Price data from Yahoo finance by downloading CSV
    def get_stock_price(ticker):
        stock = ticker
        stock_url = "https://query1.finance.yahoo.com/v7/finance/download/{}?"
        
        params = {'range':'1y','interval':'1d','events':'history'} #1 year daily stock prices
        
        response = requests.get(stock_url.format(stock), params=params)
                
        file = StringIO(response.text)
        reader = csv.reader(file)
        data = list(reader)
          
        stock_price_df  = pd.DataFrame(data[1:], columns=data[0])
        
        stock_price_df = stock_price_df.rename(columns={'Date': 'Reference Date'}) #old name to new name
        
        stock_price_df['Reference Date'] = pd.to_datetime(stock_price_df['Reference Date']).astype('datetime64[ns]').dt.date
        
        stock_price_df['Price Chg on Day'] = stock_price_df['Close'].astype(float) /stock_price_df['Open'].astype(float) - 1
        stock_price_df['T-1 Change'] = stock_price_df['Open'].astype(float).pct_change(1)
        stock_price_df['T-3 Change'] = stock_price_df['Open'].astype(float).pct_change(3)
        stock_price_df['T+1 Change'] = stock_price_df['T-1 Change'].shift(-1, axis = 0)
        stock_price_df['T+3 Change'] = stock_price_df['T-3 Change'].shift(-3, axis = 0)

        return stock_price_df
    
    stock_prices = get_stock_price(ticker)
    
    print('Stock Prices retrieved for: ', company_name,' ', ticker)
    
    
    
    try:

        story_list = []
        story_df = []
        
        #Find news titles with "company_name" in title
        search = gn.search(f'intitle:{company_name}', when = '3d')
        
        print("\n\nWebscraping: ", company_name,"\n")
        
        
        #Loop through dictionary 'search entries' to get Headline, URL etc and convert to a dataframe
        for item in search['entries']:
            story_info = []
            story_info = {'News Headline': item.title , 'URL': item.link, 'Publish Date': item.published}
            story_info['Google Search'] = company_name
            story_list.append(story_info)
            story_df = pd.DataFrame(story_list, columns =['Google Search','News Headline', 'URL','Publish Date'])
            
            #To handle weekends and bank holidays. If Saturday or Sunday move to Monday, If still blank move back to last business day on Friday
            story_df['Next Business Day'] = pd.to_datetime(story_df['Publish Date']).astype('datetime64[ns]').dt.date + BDay(0)
            story_df['Next Business Day'] = np.where(story_df['Next Business Day']=='',pd.to_datetime(story_df['Publish Date']).astype('datetime64[ns]').dt.date + BDay(-1),story_df['Next Business Day'])


    
    
        
        #Scrape articles from dataframe list story_df['URL']. Articles are limited to 300 words (Model/Hardware Limitation).
        print(f'Scraping {company_name} news links.')
        
        '''
        #scrape using beautifulsoup. This was the old version but some websites didn't work
        def scrape_and_process(URLs):
            ARTICLES = []
            for url in URLs:
                #print('Reading: ',url)
                r = requests.get(url)
                soup = BeautifulSoup(r.text, 'html.parser')
                results = soup.find_all('p')
                text = [res.text for res in results]
                words = ' '.join(text).split()[:300]
                ARTICLE = ' '.join(words)
                ARTICLES.append(ARTICLE)
            return ARTICLES
        '''
        

        #scrape using NLTK
        word_count = []
    
        def scrape_and_process(URLs):
            
            ARTICLES = []
            for url in URLs:
                try:
                    article = Article(url)
                    article.download()
                    article.parse()
                    article.nlp()
                    
                    #Get article text
                    text = article.text
                    
                    #Remove tabs and whitespace
                    text = re.sub('\s+',' ',text) 
                    words = text.split(' ')
                    
                    ARTICLE = ' '.join(words)
                    #print("\nArticle:\n",ARTICLE)
                    print('URL Article retrieved with NLTK: ', url,'\n')
                    
                except: #if NLTK doesn't work, get article with BeautifulSoup
                    r = requests.get(url)
                    soup = BeautifulSoup(r.text, 'html.parser')
                    results = soup.find_all('p')
                    text = [res.text for res in results]
                    words = ' '.join(text).split()
                    ARTICLE = ' '.join(words)
                    
                    print('URL Article Failed with NLTK so trying with BeautifulSoup: ', url,'\n')
                    
                count = len(ARTICLE.split(' '))
                print('Full Article Word count: ',count)
                word_count.append(count)
                ARTICLES.append(ARTICLE)
                
                
            return ARTICLES

        
        articles = scrape_and_process(list(story_df['URL'])) 
        
        
        
        
        big_summary = []
        ai_word_count = []
        #chunk_summarise_agg  chunks larges artiles into smaller pieces to summarise and put's it all together again
        def chunk_summarise_agg(articles):
            
            for x in articles:
                # # 3. Chunk Text
                max_chunk = 200
                
                ARTICLE = x.replace('. ', '.<eos>')
                ARTICLE = ARTICLE.replace('? ', '?<eos>')
                ARTICLE = ARTICLE.replace('! ', '!<eos>')
                
                
                sentences = ARTICLE.split('<eos>')
                current_chunk = 0 
                chunks = []
                
                for sentence in sentences:
                    if len(chunks) == current_chunk + 1: #create a new chunk +1
                        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:
                            chunks[current_chunk].extend(sentence.split(' '))
                        else:
                            current_chunk += 1 #create new chunk +1 for portion greater than max chunk
                            chunks.append(sentence.split(' '))
                    else:
                        chunks.append(sentence.split(' '))
                
                for chunk_id in range(len(chunks)):
                    chunks[chunk_id] = ' '.join(chunks[chunk_id])
                    chunks[chunk_id] = chunks[chunk_id].replace(' .', '.')
                    chunks[chunk_id] = chunks[chunk_id].replace('  ', ' ')
                    #print("\n\n\Chunks: ",chunks)
                    
            
                # # 4. Summarize Text       
                res = summarizer(chunks, max_length=80, min_length=4, do_sample=False)
                
                #' '.join([summ['summary_text'] for summ in res])
        
                text = ' '.join([summ['summary_text'] for summ in res])
                
                text = text.replace(' .', '.')
                text = text.replace('  ', ' ')
                
                print("\nChunks combined and Summarised: ",text)
                
                
                big_summary.append(text)
                
                ai_count = len(text.split(' '))
                print("Word count of AI Summary: ",ai_count)
                ai_word_count.append(ai_count)
                
                
            return big_summary
        
        chunk_summarise_agg(list(articles))
                
        
     
        
        #Summarise all Articles by passing in list of articles scraped.
        print(f'\nSummarizing {company_name} articles.')
        def summarize(articles):
            summaries = []
            for article in articles:
                input_ids = tokenizer.encode(article[:500], return_tensors="pt") #We can only summarise about 300 words at a time with Pegasus
                output = model.generate(input_ids, max_length=100,min_length=5,num_beams=5, early_stopping=True)
                summary = tokenizer.decode(output[0], skip_special_tokens=True)
                summaries.append(summary)
            return summaries
        
        
        summaries = summarize(articles)
        
        
        #Apply Sentiment Analysis
        print(f'Calculating {company_name} sentiment.')
        scores = []
        sentiment = pipeline("sentiment-analysis")
        scores = sentiment(summaries)
        
        
        #Create new dataframe model_results for full & summarised text and sentiment score.
        model_results = []
        model_results = pd.DataFrame(list(zip(articles,word_count,summaries,big_summary,ai_word_count,scores)),columns =['Full Article','Full Article Word Count','AI Headline','AI Summary','AI Summary Word Count','Sentiment Score'])
        
    
        #Combined web scraped information with model results into a new combined dataframe 
        combined = []
        combined = story_df.merge(model_results,left_index=True,right_index=True,how='outer')
        combined['Next Business Day'] = pd.to_datetime(combined['Next Business Day']).dt.date
        stock_prices['Reference Date'] = pd.to_datetime(stock_prices['Reference Date']).dt.date
        combined = pd.merge(combined, stock_prices,  how='left', left_on=['Next Business Day'], right_on = ['Reference Date']) #make sure we reference the next business day for the article so saturdays will use the prices from monday

        
        #Append to master_df with results from other companies. 
        master_df.append(combined)

        #master_df['Full Article'] = master_df['Full Article'].str.slice(0,10000)
        
    except:
        print("\n**** No Articles found for", company_name," ****\n")


master_df = pd.concat(master_df).reset_index(drop=True) 
master_df = master_df.drop(columns=['Full Article'])

master_df.to_csv("news_analytics.csv")




    





