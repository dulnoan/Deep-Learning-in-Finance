# -*- coding: utf-8 -*-
"""
Created on Thu Jan 28 11:17:23 2021

@author: micud
"""



#Download data from: https://finance.yahoo.com/quote/GOOG/history/

#PART 1. Data Pre-processing
#Step #0. Fire the system

# Import modules and packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
from datetime import datetime

from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard
import os

os.chdir(r"C:\Users\micud\Desktop\ml_testing")


#Step #1. Read data
# Importing Training Set

dataset_train = pd.read_csv('viacom.csv')
dataset_train = dataset_train.replace('#N/A N/A',0).fillna(0)
Company = "Viacom - "

n_future = 1   # Number of days we want top predict into the future
n_past = 365 #365     # Number of past days we want to use to predict the future default 90, 180 works well 

# Select features (columns) to be involved intro training and predictions. First Column is the Target variable
no_cols = dataset_train.shape[1]
cols = list(dataset_train)[1:no_cols]
BBG_FIELD = cols[0]

# Extract dates (will be used in visualization)
datelist_train = list(dataset_train['Date'])
datelist_train = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist_train]
#datelist_train = [dt.datetime.strptime(date, '%m/%d/%Y').date() for date in datelist_train]
print('Training set shape == {}'.format(dataset_train.shape))
print('All timestamps == {}'.format(len(datelist_train)))
print('Featured selected: {}'.format(cols))



#Step #2. Data pre-processing
#Removing all commas and convert data to matrix shape format.


dataset_train = dataset_train[cols].astype(str)
for i in cols:
    for j in range(0, len(dataset_train)):
        dataset_train[i][j] = dataset_train[i][j].replace(',', '')

dataset_train = dataset_train.astype(float)

# Using multiple features (predictors)
training_set = dataset_train.to_numpy() #.as_matrix()

#print('Shape of training set == {}.'.format(training_set.shape))



from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
training_set_scaled = sc.fit_transform(training_set)

sc_predict = StandardScaler()
sc_predict.fit_transform(training_set[:, 0:1])

# Creating a data structure with 90 timestamps and 1 output
X_train = []
y_train = []



for i in range(n_past, len(training_set_scaled) - n_future +1):
    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1] - 1])
    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, 0])

X_train, y_train = np.array(X_train), np.array(y_train)

print('X_train shape == {}.'.format(X_train.shape))
print('y_train shape == {}.'.format(y_train.shape))
# X_train shape == (3857, 90, 4)
# y_train shape == (3857, 1)

#PART 2. Create a model. Training
#Step #3. Building the LSTM based Neural Network

# Import Libraries and packages from Keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.optimizers import Adam
from keras.layers import GRU
from keras.layers import SimpleRNN
from keras.layers import RepeatVector

# Initializing the Neural Network based on LSTM
model = Sequential()

# Adding 1st LSTM layer
model.add(GRU(units=32, return_sequences=False, input_shape=(n_past, dataset_train.shape[1]-1))) #LSTM or GRU or SimpleRNN or SOFTMAX


# Adding LSTM layer


model.add(RepeatVector(n_past))

# Adding 2nd LSTM layer
#model.add(GRU(units=198, return_sequences=True)) #100 is good for the Hidden Layer

# Adding LSTM layer
model.add(GRU(units=32, return_sequences=True))





# Adding Dropout
model.add(Dropout(0.1))

# Output layer
model.add(Dense(units=1, activation='linear')) #'relu' 'linear' 'tanh'

model.summary()

metrics = ["accuracy"]
# Compiling the Neural Network
model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error',metrics=metrics)
# Step #4. Start training


es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=5, verbose=1)
rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)

tb = TensorBoard('logs')

history = model.fit(X_train, y_train, shuffle=True, epochs=50, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=128) #batch 256, split = 0.2 by default
#40

# PART 3. Make future predictions

# Generate list of sequence of days for predictions
datelist_future = pd.date_range(datelist_train[-1], periods=n_future, freq='1d').tolist()

'''
Remeber, we have datelist_train from begining.
'''

# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE
datelist_future_ = []
for this_timestamp in datelist_future:
    datelist_future_.append(this_timestamp.date())
# Step #5. Make predictions for future dates

# Perform predictions
predictions_future = model.predict(X_train[-n_future:])

predictions_train = model.predict(X_train[n_past:])




# Inverse the predictions to original measurements

# ---> Special function: convert <datetime.date> to <Timestamp>
def datetime_to_timestamp(x):
    '''
        x : a given datetime value (datetime.date)
    '''
    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')


y_pred_future = sc_predict.inverse_transform(predictions_future)
y_pred_train = sc_predict.inverse_transform(predictions_train)



BBG_FIELD = cols[0]

PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=[BBG_FIELD]).set_index(pd.Series(datelist_future))
PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=[BBG_FIELD]).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))


# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN
PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)


# Step #6. Visualize the Predictions

# Set plot size 
from pylab import rcParams
rcParams['figure.figsize'] = 21,10

# Plot parameters
START_DATE_FOR_PLOTTING = datelist_train[0]
END_DATE_FOR_PLOTTING = datelist_train[-1]



    

# Parse training set timestamp for better visualization
dataset_train = pd.DataFrame(dataset_train, columns=cols)
dataset_train.index = datelist_train
dataset_train.index = pd.to_datetime(dataset_train.index)



# Sentiment Analysis Line
PREDICTION_TRAIN_SENTIMENT = PREDICTION_TRAIN.rename({BBG_FIELD:"Prediction"}, axis =1 )
sentiment = pd.concat([PREDICTION_TRAIN_SENTIMENT,dataset_train], axis=1)
sentiment = sentiment.fillna(0)
sentiment["Sentiment1"] = (sentiment["PX_LAST"] - sentiment["Prediction"])

sentiment["Sentiment"] = np.where(sentiment["Sentiment1"] != sentiment["PX_LAST"], sentiment["Sentiment1"], 0) #/sentiment["PX_LAST"]  #- min(sentiment["Sentiment1"])) / (max(sentiment["Sentiment1"]) - min(sentiment["Sentiment1"])) * 10

sentiment["Sentiment"]  = sentiment["Sentiment"][sentiment["Sentiment"]!= 0.000001]
latest_sentiment = round(sentiment["Sentiment"][-1],2)
sentiment.index = sentiment.index.to_series().apply(datetime_to_timestamp)


error_vs_actual = round(min(history.history['val_loss']),2)
train_error_vs_actual = round(min(history.history['loss']),2)
rank = history.history['val_loss'].index(min(history.history['val_loss']))
#START_DATE_FOR_PLOTTING = "2021-01-01"
def plt_prediction():
    
    fig, axs = plt.subplots(3)
    #plt.plot(PREDICTIONS_FUTURE.loc[START_DATE_FOR_PLOTTING:].index, PREDICTIONS_FUTURE.loc[START_DATE_FOR_PLOTTING:][BBG_FIELD], color='r', label='Feature Interpolation')
    axs[0].plot(PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:].index, PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:][BBG_FIELD], color='orange', label=f'Deep Learning in terms of {BBG_FIELD}')
    axs[0].plot(dataset_train.loc[START_DATE_FOR_PLOTTING:].index, dataset_train.loc[START_DATE_FOR_PLOTTING:][BBG_FIELD], color='b', label=f'Actual {BBG_FIELD}')
 
    
    axs[0].axvline(x = min(PREDICTIONS_FUTURE.index), color='green', linewidth=2, linestyle='--')
    axs[0].grid(which='major', color='#cccccc', alpha=0.5) 
    axs[0].legend(shadow=True)
    axs[0].set_title(f'{Company} Deep Learning vs Acutal {START_DATE_FOR_PLOTTING} to {END_DATE_FOR_PLOTTING}', family='Arial', fontsize=12)
    axs[0].set_xlabel('Timeline', family='Arial', fontsize=12)
    axs[0].set_ylabel(f'{BBG_FIELD}', family='Arial', fontsize=12)


    axs[1].set_ylabel('Unexplained Sentiment',fontsize=12)  # we already handled the x-label with ax1
    axs[1].plot(sentiment.loc[START_DATE_FOR_PLOTTING:].index, sentiment.loc[START_DATE_FOR_PLOTTING:]["Sentiment"], color='purple', label=f'Unexplained Sentiment in terms of {BBG_FIELD}: {latest_sentiment }')    
    #axs[1].set_title(f'Unexplained Sentiment Indicator in terms of {BBG_FIELD}')
    axs[1].grid(which='major', color='#cccccc', alpha=0.5) 
    axs[1].legend(shadow=True)
    
    axs[2].plot(history.history["val_loss"], label=f"Validation Performance MSE", color='orange')
    axs[2].plot(history.history["loss"], label=f"Training Performance MSE", color='b',)
    axs[2].axvline(x = rank, color='green', linewidth=2, linestyle='--', label= f"Best Epoch: {rank}, MSE: {error_vs_actual}%")

    axs[2].set_ylabel("Error",fontsize=12)
    axs[2].set_xlabel("Epoch - Iteration",fontsize=12)
    axs[2].legend(loc="upper left")
    axs[2].grid(which='major', color='#cccccc', alpha=0.5) 
    axs[2].set_title("Training Performance - Mean Squared Error")  
    axs[2].legend(shadow=True)
    

    
    # axs[1].plot(history.history["val_loss"], label=f"Validation Performance MSE", color='orange')
    # axs[1].plot(history.history["loss"], label=f"Training Performance MSE", color='b',)
    # axs[1].axvline(x = rank, color='green', linewidth=2, linestyle='--', label= f"Best Epoch: {rank}, MSE: {error_vs_actual}%")

    # axs[1].set_ylabel("Error",fontsize=12)
    # axs[1].set_xlabel("Epoch - Iteration",fontsize=12)
    # axs[1].legend(loc="upper left")
    # axs[1].grid(which='major', color='#cccccc', alpha=0.5) 
    # #axs[1].set_title("Training Performance - Mean Squared Error")  
    # axs[1].legend(shadow=True)

    plt.show()


plt_prediction()




model.summary()



    
    
