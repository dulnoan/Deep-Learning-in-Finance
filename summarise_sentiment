# 1. Install and Import Baseline Dependencies

from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from bs4 import BeautifulSoup
import requests
import re
from transformers import pipeline
import pandas as pd
import randomheaders



# 2. Setup Model
model_name = "human-centered-summarization/financial-summarization-pegasus"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

# 3. Setup Pipeline
monitored_tickers = ['BLK','MVIS']



# 4.1. Search for Stock News using Google and Yahoo Finance
print('Searching for stock news for', monitored_tickers)
def search_for_stock_news_links(ticker):
    search_url = 'https://www.google.com/search?q=yahoo+finance+{}&tbm=nws'.format(ticker) #'https://www.google.com/search?q=yahoo+finance+{}&tbm=nws'
    print(search_url)
    r = requests.get(search_url, headers=randomheaders.LoadHeader(),timeout=30)
    #r = requests.get(search_url)
    #r.text = this will show everything on the URL, Javascript and CSS etc.
    soup = BeautifulSoup(r.text, 'html.parser') #we passed in text and we use html.parser to get text body only
    atags = soup.find_all('a') # "a" tags are used to scrape URLs
    #print(atags)
    hrefs = [link['href'] for link in atags]
    return hrefs

raw_urls = {ticker:search_for_stock_news_links(ticker) for ticker in monitored_tickers} # this is a for loop, first part is to create a dictionary key = "ticker", value = "search_for_stock_news_links"

# raw_urls
for ticker in monitored_tickers:
    raw_urls[ticker] = search_for_stock_news_links(ticker)

#test = search_for_stock_news_links('ROO') 




# 4.2. Strip out unwanted URLs
print('Cleaning URLs.')
exclude_list = ['maps', 'policies', 'preferences', 'accounts', 'support','www.yahoo.com']
def strip_unwanted_urls(urls, exclude_list):
    val = []
    for url in urls:
        if 'https://' in url and not any(exc in url for exc in exclude_list):
            res = re.findall(r'(https?://\S+)', url)[0].split('&')[0]
            val.append(res)
    return list(set(val))

cleaned_urls = {ticker:strip_unwanted_urls(raw_urls[ticker] , exclude_list) for ticker in monitored_tickers} 



# 4.3. Search and Scrape Cleaned URLs
print('Scraping news links.')

def scrape_and_process(URLs):
    ARTICLES = []
    for url in URLs:
        r = requests.get(url)
        r.text
        soup = BeautifulSoup(r.text, 'html.parser')
        results = soup.find_all('p')
        text = [res.text for res in results] # list comprehension where res.text is the text from URL
        words = ' '.join(text).split('')[:350] #This gets the first x number of words. The code joins all words to make a big string then taking first "x" number of words 
        #updated from ' ' to '' to remove all whitespace
        print("Number of Words in Paragraph: ",len(words))
        ARTICLE = ' '.join(words) #We put the words back together
        ARTICLES.append(ARTICLE)
    return ARTICLES
articles = {ticker:scrape_and_process(cleaned_urls[ticker]) for ticker in monitored_tickers} 





# 4.4. Summarise all Articles
print('Summarizing articles.')
def summarize(articles):
    summaries = []
    for article in articles:
        input_ids = tokenizer.encode(article, return_tensors="pt") # "pt" is pytorch tensor. Converts each words to a Numeric ID
        output = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)
        summary = tokenizer.decode(output[0], skip_special_tokens=True)
        print(summary)
        summaries.append(summary)
    return summaries

summaries = {ticker:summarize(articles[ticker]) for ticker in monitored_tickers}




# 5. Adding Sentiment Analysis
print('Calculating sentiment.')

sentiment = pipeline("sentiment-analysis")

scores = {ticker:sentiment(summaries[ticker]) for ticker in monitored_tickers}





# # 6. Exporting Results
print('Exporting results.')
def create_output_array(summaries, scores, urls):
    output = []
    for ticker in monitored_tickers:
        for counter in range(len(summaries[ticker])):
            output_this = [
                            ticker, 
                            summaries[ticker][counter], 
                            scores[ticker][counter]['label'], 
                            scores[ticker][counter]['score'], 
                            urls[ticker][counter]
                          ]
            output.append(output_this)
    return output

#final_output = create_output_array(summaries, scores, cleaned_urls)
#final_output.insert(0, ['Ticker','Summary', 'Sentiment', 'Sentiment Score', 'URL'])

#x = test_scores #[0] to get first dictionary in the list, ['label'] the key for the value you ar after -> test_scores[0]['label']
#print(x)


# with open('ethsummaries.csv', mode='w', newline='') as f:
#     csv_writer = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
#     csv_writer.writerows(final_output)





