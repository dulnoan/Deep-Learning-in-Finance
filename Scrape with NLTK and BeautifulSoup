# -*- coding: utf-8 -*-
"""
Created on Sun May 23 09:15:01 2021

@author: micud


url = 'https://www.fool.co.uk/investing/2021/05/21/the-trainline-share-price-steadies-after-thursdays-20-crash-should-i-buy/'

article = Article(url)

#Do some NLP
article.download()
article.parse()

article.nlp()

#Get article text
article_text = article.text

#Get the author
author_list = article.authors

#Get publish date
publish_date = article.publish_date

#Get top image
image = article.top_image



Key packages used:
pygooglenews = reliable package to scrape URL results to overcome beautiful soup issues on my end. https://github.com/kotartemiy/pygooglenews
beautifulsoup = used to scrape text in webpages
nltk = 
financial-summarization-pegasus = Deep Learning model that has been pretrained specifically for Financial summarization

"""
import nltk
from newspaper import Article
#nltk.download('punkt')
nltk.download('punkt')
import pandas as pd
from bs4 import BeautifulSoup
import requests
from pygooglenews import GoogleNews
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from transformers import pipeline
import re

#Specify Watchlist
watchlist = ['Sumo Digital','Ocado','IQE','Trainline']



#Specify Region
gn = GoogleNews(country = 'UK')



#Setup AI Model using Pegasus which has been trained for Financial Summarization
model_name = "human-centered-summarization/financial-summarization-pegasus"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)
summarizer = pipeline("summarization")



#This will be the final dataframe output where results for each company will be appended
master_df = []

for company_name in watchlist:
    
    #company_name = 'Ocado'
    
    try:

        story_list = []
        story_df = []
        
        #Find news titles with "company_name" in title
        search = gn.search(f'intitle:{company_name}', when = '3d')
        
        print("\n\nWebscraping: ", company_name,"\n")
        
        
        #Loop through dictionary 'search entries' to get Headline, URL etc and convert to a dataframe
        for item in search['entries']:
            story_info = []
            story_info = {'Search Headline': item.title , 'URL': item.link}
            story_info['Company'] = company_name
            story_list.append(story_info)
            story_df = pd.DataFrame(story_list, columns =['Company','Search Headline', 'URL'])
    
    
        
        #Scrape articles from dataframe list story_df['URL']. Articles are limited to 300 words (Model/Hardware Limitation).
        print(f'Scraping {company_name} news links.')
        
        '''
        #scrape using beautifulsoup. This was the old version but some websites didn't work
        def scrape_and_process(URLs):
            ARTICLES = []
            for url in URLs:
                #print('Reading: ',url)
                r = requests.get(url)
                soup = BeautifulSoup(r.text, 'html.parser')
                results = soup.find_all('p')
                text = [res.text for res in results]
                words = ' '.join(text).split()[:300]
                ARTICLE = ' '.join(words)
                ARTICLES.append(ARTICLE)
            return ARTICLES
        '''
        

        #scrape using NLTK
    
        def scrape_and_process(URLs):
            ARTICLES = []
            for url in URLs:
                try:
                    article = Article(url)
                    article.download()
                    article.parse()
                    article.nlp()
                    
                    #Get article text
                    text = article.text
                    
                    #Remove tabs and whitespace
                    text = re.sub('\s+',' ',text) 
                    words = text.split(' ')
                    
                    ARTICLE = ' '.join(words)
                    #print("\nArticle:\n",ARTICLE)
                    print('URL Article retrieved with NLTK: ', url,'\n')
                    
                except: #if NLTK doesn't work, get article with BeautifulSoup
                    r = requests.get(url)
                    soup = BeautifulSoup(r.text, 'html.parser')
                    results = soup.find_all('p')
                    text = [res.text for res in results]
                    words = ' '.join(text).split()
                    ARTICLE = ' '.join(words)
                    
                    print('URL Article Failed with NLTK so trying with BeautifulSoup: ', url,'\n')
                    
                    
                ARTICLES.append(ARTICLE)
                
            return ARTICLES

        
        articles = scrape_and_process(list(story_df['URL'])) 
        
        
        
        
        big_summary = []
        
        
        def chunk_summarise_agg(articles):
            
            for x in articles:
                # # 3. Chunk Text
                max_chunk = 200
                
                ARTICLE = x.replace('. ', '.<eos>')
                ARTICLE = ARTICLE.replace('? ', '?<eos>')
                ARTICLE = ARTICLE.replace('! ', '!<eos>')
                
                
                sentences = ARTICLE.split('<eos>')
                current_chunk = 0 
                chunks = []
                
                for sentence in sentences:
                    if len(chunks) == current_chunk + 1: #create a new chunk +1
                        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:
                            chunks[current_chunk].extend(sentence.split(' '))
                        else:
                            current_chunk += 1 #create new chunk +1 for portion greater than max chunk
                            chunks.append(sentence.split(' '))
                    else:
                        chunks.append(sentence.split(' '))
                
                for chunk_id in range(len(chunks)):
                    chunks[chunk_id] = ' '.join(chunks[chunk_id])
                    chunks[chunk_id] = chunks[chunk_id].replace(' .', '.')
                    chunks[chunk_id] = chunks[chunk_id].replace('  ', ' ')
                    print("\n\n\Chunks: ",chunks)
                    
            
                # # 4. Summarize Text       
                res = summarizer(chunks, max_length=80, min_length=4, do_sample=False)
                
                #' '.join([summ['summary_text'] for summ in res])
        
                text = ' '.join([summ['summary_text'] for summ in res])
                
                text = text.replace(' .', '.')
                text = text.replace('  ', ' ')
                
                print("\ntext combined: ",text)
                
                big_summary.append(text)
                
            return big_summary
        
        chunk_summarise_agg(list(articles))
                
        
     
        
        #Summarise all Articles by passing in list of articles scraped.
        print(f'Summarizing {company_name} articles.')
        def summarize(articles):
            summaries = []
            for article in articles:
                input_ids = tokenizer.encode(article[:350], return_tensors="pt") #We can only summarise about 300 words at a time with Pegasus
                output = model.generate(input_ids, max_length=100,min_length=5,num_beams=5, early_stopping=True)
                summary = tokenizer.decode(output[0], skip_special_tokens=True)
                summaries.append(summary)
            return summaries
        
        
        summaries = summarize(articles)
        
        
        #Apply Sentiment Analysis
        print(f'Calculating {company_name} sentiment.')
        scores = []
        sentiment = pipeline("sentiment-analysis")
        scores = sentiment(summaries)
        
        
        #Create new dataframe model_results for full & summarised text and sentiment score.
        model_results = []
        model_results = pd.DataFrame(list(zip(articles,summaries,big_summary,scores)),columns =['Full Article','AI Headline','AI Summary', 'Sentiment Score'])
        
    
        #Combined web scraped information with model results into a new combined dataframe 
        combined = []
        combined = story_df.merge(model_results,left_index=True,right_index=True,how='outer')
        
        #Append to master_df with results from other companies. 
        master_df.append(combined)
        
    except:
        print("\n**** No articles for ", company_name," ****\n")

if len(master_df) > 1:
    master_df = pd.concat(master_df)
    master_df.to_excel("scrape_results.xls")

else:
    master_df.to_excel("scrape_results.xls")

