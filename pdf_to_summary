# -*- coding: utf-8 -*-
"""
This convers PDF's into a long summary

"""
import os
import re
from transformers import pipeline
from bs4 import BeautifulSoup
import requests
import torch
torch.cuda.empty_cache()
print(torch.cuda.is_available())

os.chdir(r"C:\Users\micud\Desktop\pdf")


filename = 'trainline.pdf'


import PyPDF2

master_text = []
with open(filename,'rb') as pdf_file, open('pdf_sample.txt', 'w') as text_file:
    read_pdf = PyPDF2.PdfFileReader(pdf_file)
    number_of_pages = read_pdf.getNumPages()
    for page_number in range(number_of_pages):   # use xrange in Py2
        page = read_pdf.getPage(page_number)
        page_content = page.extractText()
        page_content = page_content.replace('\n','')
        text_file.write(page_content)
        master_text.append(page_content)
    
ARTICLE = ' '.join(master_text)

'''
# This is if you want to read in a text file
with open('ocado.txt', encoding='utf8',errors = 'ignore') as f:
    ARTICLE = f.read()
'''




# # 1. Load Summarization Pipeline
summarizer = pipeline("summarization")



#ARTICLE
#print("Full Article Word Count: ",len(ARTICLE))


# # 3. Chunk Text

ARTICLE = ARTICLE.replace('  ', '.<eos>')
ARTICLE = ARTICLE.replace('. ', '.<eos>')
ARTICLE = ARTICLE.replace('?', '?<eos>')
ARTICLE = ARTICLE.replace('!', '!<eos>')

#ARTICLE = ARTICLE[:500000] #Limit full Article to X characters

sentences = ARTICLE.split('<eos>')

print("Original number of Sentences: ",len(sentences))

sentences = sentences[:3000] #Limit input to X number of sentences

print("Number of sentences limited to: ",len(sentences))



max_chunk = 512 #  i.e for every 500 words (max_chunk) we summarise into 70 (max_length)
current_chunk = 0 
chunks = []

for sentence in sentences:

    if len(chunks) == current_chunk + 1: #create a new chunk +1
        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:
            chunks[current_chunk].extend(sentence.split(' '))
            #print("Chunk word count: ",len(chunks[current_chunk]))
        else:
            current_chunk += 1 #create new chunk +1 for portion greater than max chunk
            chunks.append(sentence.split(' '))
            #print("Chunk word count: ",len(chunks[current_chunk]))
    else:
        chunks.append(sentence.split(' '))
        #print("Chunk word count: ",len(chunks[current_chunk]))
        


for chunk_id in range(len(chunks)):
    chunks[chunk_id] = ' '.join(chunks[chunk_id])
    



# # 4. Summarize Text
print("RUNNING SUMMARIZER PIPELINE")
res = summarizer(chunks, max_length=80, min_length=10, do_sample=False)

#res[0].keys()  to see what keys i.e 'summary_text' to get the value/body of text
#res[0](['summary_text'])
#' '.join([summ['summary_text'] for summ in res])


text = ' '.join([summ['summary_text'] for summ in res])

print("Final Summarized word count: ", len(text))
print("Summary vs Original: ", round(len(text)/len(ARTICLE)*100,2),"%")



# # 5. Output to Text File

with open('annual_report_summary.txt', 'w', encoding='utf-8') as f:
    f.write(text)




########## Summary of Summary SECOND STAGE #######################

# # 3. Chunk Text

SUMMARY = text.replace('  ', '.<eos>')
SUMMARY = SUMMARY.replace(' . ', '.<eos>')
SUMMARY = SUMMARY.replace('? ', '?<eos>')
SUMMARY = SUMMARY.replace('! ', '!<eos>')

#ARTICLE = ARTICLE[:500000] #Limit full Article to X characters

sentences = SUMMARY.split('<eos>')

print("Original number of Sentences: ",len(sentences))

sentences = sentences[:25] #Limit input to X number of sentences

print("Number of sentences limited to: ",len(sentences))



max_chunk = 250 #  i.e for every 250 words (max_chunk) we summarise into 70 (max_length)
current_chunk = 0 
chunks = []

for sentence in sentences:

    if len(chunks) == current_chunk + 1: #create a new chunk +1
        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:
            chunks[current_chunk].extend(sentence.split(' '))
            #print("Chunk word count: ",len(chunks[current_chunk]))
        else:
            current_chunk += 1 #create new chunk +1 for portion greater than max chunk
            chunks.append(sentence.split(' '))
            #print("Chunk word count: ",len(chunks[current_chunk]))
    else:
        chunks.append(sentence.split(' '))
        #print("Chunk word count: ",len(chunks[current_chunk]))
        


for chunk_id in range(len(chunks)):
    chunks[chunk_id] = ' '.join(chunks[chunk_id])
    



# # 4. Summarize Text
print("RUNNING SECOND SUMMARIZER PIPELINE")
res = summarizer(chunks, max_length=200, min_length=10, do_sample=False)

#res[0].keys()  to see what keys i.e 'summary_text' to get the value/body of text
#res[0](['summary_text'])
#' '.join([summ['summary_text'] for summ in res])


SUMMARY_FINAL = ' '.join([summ['summary_text'] for summ in res])

print("Final Second Summarized word count: ", len(SUMMARY_FINAL))
print("Summary vs Original: ", round(len(SUMMARY_FINAL)/len(ARTICLE)*100,2),"%")



# # 5. Output to Text File

with open('headline.txt', 'w', encoding='utf-8') as f:
    f.write(SUMMARY_FINAL)
